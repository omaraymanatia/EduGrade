{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15239b22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:  20%|██        | 1/5 [05:42<22:51, 342.78s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from huggingface_hub import login\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Use 4-bit quantization (smallest memory footprint)\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "login(token=\"your_huggingface_token\")  # Replace with your Hugging Face token\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\",\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a64de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = '/kaggle/input/exam-images/deep1.jpg'\n",
    "prompt = \"\"\"Convert the provided exam content into a strict JSON matching the specified SQL schema. Follow these rules:\n",
    "\n",
    "1. **For MCQs (True/False or Multiple Choice):**  \n",
    "   - Map all options with `isCorrect: false` by default (never answer for the student).  \n",
    "   - If the question includes a pre-selected answer, mark `isCorrect: true` for that option.  \n",
    "   - If no answer is provided, leave `options` as an empty list.  \n",
    "\n",
    "2. **Question Types:**  \n",
    "   - `mcq`: For True/False or multiple-choice questions.  \n",
    "   - `essay`: For open-ended questions (no options).  \n",
    "\n",
    "3. **Required JSON Structure:**  \n",
    "```json\n",
    "{\n",
    "  \"exam\": {\n",
    "    \"title\": \"[Exam Title]\",\n",
    "    \"courseCode\": \"[Course Code]\",\n",
    "    \"institution\": \"[Institution Name]\",\n",
    "    \"faculty\": \"[Faculty Name]\",\n",
    "    \"level\": \"[Academic Level]\",\n",
    "    \"major\": \"[Major/Field]\",\n",
    "    \"date\": \"[Exam Date]\",\n",
    "    \"duration\": [Duration in Minutes],\n",
    "    \"totalMarks\": [Total Marks],\n",
    "    \"passingScore\": [Passing Threshold],\n",
    "    \"examiner\": \"[Examiner Name]\",\n",
    "    \"instructions\": \"Answer the following questions according to your study\",\n",
    "    \"questions\": [\n",
    "      {\n",
    "        \"id\": 1,\n",
    "        \"text\": \"Google File System (GFS) is designed for small data-intensive applications.\",\n",
    "        \"type\": \"mcq\",\n",
    "        \"points\": 1,\n",
    "        \"order\": 1,\n",
    "        \"options\": [\n",
    "          { \"text\": \"True\", \"isCorrect\": false, \"order\": 1 },\n",
    "          { \"text\": \"False\", \"isCorrect\": false, \"order\": 2 }\n",
    "        ]\n",
    "      },\n",
    "      ...\n",
    "      {\n",
    "        \"id\": 14,\n",
    "        \"text\": \"If you have six processes from D1 to D2, apply (ring algorithm) to detect the coordinator of the ring, known that D1 discovers that the coordinator D2 is dead, but will start election (you must draw each step with brief description).\",\n",
    "        \"type\": \"essay\",\n",
    "        \"points\": 1,\n",
    "        \"order\": 14,\n",
    "        \"options\": []\n",
    "      }\n",
    "      ... \n",
    "    ]\n",
    "  }\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e85691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = '/kaggle/output'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to process image and generate JSON\n",
    "def process_exam_image(image_path, question_prompt):\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"image\", \"image\": image_path}, {\"type\": \"text\", \"text\": question_prompt}]}]\n",
    "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(text=[text], images=image_inputs, videos=video_inputs, padding=True, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(\"cuda\")\n",
    "    \n",
    "    # Generate response without token limit constraint\n",
    "    generated_ids = model.generate(**inputs, max_length=model.config.max_position_embeddings)\n",
    "    \n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    \n",
    "    return output_text[0]\n",
    "\n",
    "# Process the image\n",
    "result_text = process_exam_image(image, prompt)\n",
    "result_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6aac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Try to parse the result as JSON\n",
    "try:\n",
    "    # Strip any potential markdown code block formatting\n",
    "    if \"```json\" in result_text:\n",
    "        json_text = result_text.split(\"```json\")[1].split(\"```\")[0].strip()\n",
    "    elif \"```\" in result_text:\n",
    "        json_text = result_text.split(\"```\")[1].strip()\n",
    "    else:\n",
    "        json_text = result_text.strip()\n",
    "    \n",
    "    # Parse to validate JSON structure\n",
    "    result_json = json.loads(json_text)\n",
    "    \n",
    "    # Save to output file\n",
    "    output_path = os.path.join(output_dir, 'exam_results.json')\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(result_json, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Successfully saved JSON output to {output_path}\")\n",
    "    \n",
    "    # Print a preview of the JSON\n",
    "    print(\"\\nJSON Preview (first few questions):\")\n",
    "    questions = result_json.get('exam', {}).get('questions', [])\n",
    "    preview_count = min(3, len(questions))\n",
    "    preview = {\n",
    "        \"exam\": {\n",
    "            \"title\": result_json.get('exam', {}).get('title', ''),\n",
    "            \"questions\": questions[:preview_count]\n",
    "        }\n",
    "    }\n",
    "    print(json.dumps(preview, indent=2))\n",
    "    print(f\"\\nTotal questions processed: {len(questions)}\")\n",
    "    \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error parsing JSON: {e}\")\n",
    "    print(\"Raw output (may not be valid JSON):\")\n",
    "    print(result_text)\n",
    "    \n",
    "    # Save the raw output anyway\n",
    "    output_path = os.path.join(output_dir, 'exam_results_raw.txt')\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(result_text)\n",
    "    print(f\"Saved raw output to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
